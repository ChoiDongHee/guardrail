import re
import logging
from typing import List, Dict, Tuple, Optional
import json
import os
from dotenv import load_dotenv
from guardrail_cache_v2.services.redis_service import EnhancedRedisManager


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)



load_dotenv()

#SYNONYM_DICTS = json.loads(os.getenv('SYNONYM_DICTS'))
STOPWORDS_DICTS = json.loads(os.getenv('STOPWORDS_DICTS'))





def build_stopwords_set(dicts: List[Dict]) -> set:
    stopwords = set()
    for group in dicts:
        stopwords.update(group["words"])
    return stopwords

# -----------------------------
# âœ… í˜•íƒœì†Œ ê¸°ë°˜ ì „ì²˜ë¦¬ í´ë˜ìŠ¤
# -----------------------------
class RefinedKoreanPreprocessor:
    def __init__(self,synonym_dict: Optional[List[Dict[str, List[str]]]] = None, stopwords: Optional[List[str]] = None, meaningful_pos: List[str] = None):
        try:
            import MeCab
            self.tagger = MeCab.Tagger()
            logger.info("âœ… MeCab ë¶„ì„ê¸° ë¡œë”© ì™„ë£Œ")
        except ImportError:
            try:
                import mecab_ko as MeCab
                self.tagger = MeCab.Tagger()
                logger.info("âœ… mecab_ko ëŒ€ì²´ ë¡œë”© ì™„ë£Œ")
            except ImportError as e:
                logger.exception("âŒ MeCab ë¶„ì„ê¸° ë¡œë”© ì‹¤íŒ¨")
                raise ImportError("mecab-ko ì„¤ì¹˜ í•„ìš”: pip install mecab-ko") from e
        self.synonym_dict = synonym_dict if synonym_dict else []
        self.stopwords = set(stopwords) if stopwords else build_stopwords_set(STOPWORDS_DICTS)
        self.meaningful_pos = set(meaningful_pos if meaningful_pos else [
            'NNG', 'NNP', 'NNB', 'VV', 'VA', 'MM', 'MAG', 'SL', 'SN', 'SH'
        ])

    def update_synonym_dict(self, synonym_dicts_json: str) -> bool:
        """
        ì™¸ë¶€ì—ì„œ SYNONYM_DICTS JSONì„ ë°›ì•„ì„œ self.synonym_dictë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜

        Args:
            synonym_dicts_json (str): JSON ë¬¸ìì—´ í˜•íƒœì˜ ë™ì˜ì–´ ì‚¬ì „
                                     ì˜ˆ: '[{"representative": "íŒŒì´ì¬", "synonyms": ["python", "Python"]}]'

        Returns:
            bool: ì—…ë°ì´íŠ¸ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info("ğŸ”„ ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì‹œì‘")

            # JSON ë¬¸ìì—´ì„ íŒŒì‹±
            if isinstance(synonym_dicts_json, str):
                synonym_data = json.loads(synonym_dicts_json)
            elif isinstance(synonym_dicts_json, list):
                # ì´ë¯¸ íŒŒì‹±ëœ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                synonym_data = synonym_dicts_json
            else:
                raise ValueError("ì…ë ¥ ë°ì´í„°ëŠ” JSON ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")

            # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
            if not isinstance(synonym_data, list):
                raise ValueError("ë™ì˜ì–´ ë°ì´í„°ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")

            for entry in synonym_data:
                if not isinstance(entry, dict):
                    raise ValueError("ê° ë™ì˜ì–´ í•­ëª©ì€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
                if "representative" not in entry or "synonyms" not in entry:
                    raise ValueError("ê° í•­ëª©ì€ 'representative'ì™€ 'synonyms' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤")
                if not isinstance(entry["synonyms"], list):
                    raise ValueError("'synonyms'ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")

            # ê¸°ì¡´ ë™ì˜ì–´ ì‚¬ì „ ë°±ì—… (ë¡¤ë°±ìš©)
            old_synonym_dict = self.synonym_dict.copy() if self.synonym_dict else []

            # ìƒˆë¡œìš´ ë™ì˜ì–´ ì‚¬ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            self.synonym_dict = synonym_data

            logger.info(f"âœ… ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì™„ë£Œ - ì´ {len(self.synonym_dict)}ê°œ ê·¸ë£¹")

            # ì—…ë°ì´íŠ¸ëœ ë‚´ìš© ë¡œê¹…
            for entry in self.synonym_dict:
                logger.debug(f"  - ëŒ€í‘œì–´: {entry['representative']}, ë™ì˜ì–´ ìˆ˜: {len(entry['synonyms'])}")

            return True

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return False

        except ValueError as e:
            logger.error(f"âŒ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

        except Exception as e:
            logger.error(f"âŒ ë™ì˜ì–´ ì‚¬ì „ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            # ë¡¤ë°± ì‹œë„
            try:
                if 'old_synonym_dict' in locals():
                    self.synonym_dict = old_synonym_dict
                    logger.info("ğŸ”„ ì´ì „ ë™ì˜ì–´ ì‚¬ì „ìœ¼ë¡œ ë¡¤ë°± ì™„ë£Œ")
            except:
                logger.error("âŒ ë¡¤ë°±ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            return False



    def clean_text(self, text: str) -> str:
        try:
            return re.sub(r'\s+', ' ', text.strip())
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ì •ì œ ì‹¤íŒ¨: {e}")
            return text

    def parse_mecab(self, text: str) -> List[Tuple[str, str]]:
        try:
            logger.debug(f"í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘: {text}")
            result = self.tagger.parse(text)
            morphemes = []
            for line in result.split('\n'):
                if line == 'EOS' or not line:
                    continue
                parts = line.split('\t')
                if len(parts) >= 2:
                    morpheme = parts[0]
                    feature = parts[1].split(',')
                    pos = feature[0]
                    lemma = feature[6] if len(feature) > 6 and feature[6] != '*' else morpheme
                    morphemes.append((lemma, pos))
            logger.debug(f"í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼(í‘œì œì–´): {morphemes}")
            return morphemes
        except Exception as e:
            logger.exception(f"âŒ í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []

    def extract_keywords(self, text: str) -> List[str]:
        try:
            logger.info("ğŸš€ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
            # rep_map = self.build_synonym_map()
            # pre_clean_text = self.clean_text(text)
            #
            # normalized = [rep_map.get(word, word) for word in pre_clean_text]
            #morphemes = self.parse_mecab(normalized)
            morphemes = self.parse_mecab(self.clean_text(text))
            keywords = [
                word for word, pos in morphemes
                if pos in self.meaningful_pos
                and word not in self.stopwords
                and len(word) >= 2
                and not re.match(r'^[a-zA-Z]$', word)
            ]
            logger.info(f"ğŸ” ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            # âœ… ëŒ€í‘œì–´ ì¹˜í™˜ ì¶”ê°€
            rep_map = self.build_synonym_map()
            normalized = [rep_map.get(word, word) for word in keywords]

            logger.info(f"ğŸ§¹ ë™ì˜ì–´+ëŒ€í‘œì–´ ì¹˜í™˜ ì¶”ê°€ ì •ë¦¬ ê²°ê³¼: {normalized}")
            return normalized

        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def build_synonym_map(self) -> Dict[str, str]:
        rep_map = {}
        try:
            for entry in self.synonym_dict:
                rep = entry["representative"]
                for word in entry["synonyms"]:
                    rep_map[word] = rep
            return rep_map
        except Exception as e:
            logger.error(f"âŒ ë™ì˜ì–´ ë§µ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def apply_synonym_replacement(self, keywords: List[str], rep_map: Dict[str, str]) -> List[str]:
        try:
            normalized = [rep_map.get(word, word) for word in keywords]
            logger.info(f"ğŸ§¹ ë™ì˜ì–´ ì •ë¦¬ ê²°ê³¼: {normalized}")
            return normalized
        except Exception as e:
            logger.error(f"âŒ ë™ì˜ì–´ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return keywords

    def join_keywords(self, keywords: List[str]) -> str:
        try:
            joined = ' '.join(keywords)
            logger.info(f"ğŸ“ í‚¤ì›Œë“œ ìŠ¤íŠ¸ë§ ë³€í™˜: {joined}")
            return joined
        except Exception as e:
            logger.error(f"âŒ í‚¤ì›Œë“œ ìŠ¤íŠ¸ë§ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ''



